---
# layout: post
permalink: meaningweaver
title: Meaning Weaver
nav_order: 95
---


# Weaving "Universal" Meaning from Numbers

While a few of the most prominent names from eugenic "race science" and military projects have been stripped from buildings or removed from lecture posts, their methods and logics carry forward into the latest generative AI systems. Francis Galton, Ronald Fisher, Karl Pearson's, and others' racist, capitalist, militarizing worldviews are embedded into the way we think with numbers [and the way we numericalize everything] today. As statistics removes the names of Pearson, Fisher, and Galton, machine learning chugs on and science still measures the right to publish by their idea of "statistical significance." @claytonHowEugenicsShaped2020

Oft repeated “the meaning of a word can be inferred by the company it keeps” JR FIRTH. >>> 

WENDY CHUN Homophily

“Distributional hypothesis in semantics” words more similar will be used more often together
https://www.youtube.com/watch?v=oUpuABKoElw
JR Firth (1957)

What would happen to use unicode numbers as arbitrary token index starting point for byte pair encoding rather than frequency score?
ASCII developed from telegraphic codes. Its first commercial use was as a seven-bit teleprinter code promoted by Bell data services. Work on ASCII formally began October 6, 1960

GPT: SMASHWORDS is original BOOKCORPUS source
https://huggingface.co/datasets/bookcorpus
*Addressing "Documentation Debt" in Machine Learning Research: A Retrospective Datasheet for BookCorpus


VOCABULARY VECTORIZING EMBEDDINGS
Pre-Training a vocab: “Typically the vocabulary is created from training data by retaining the most frequent N words in the source and target language." (https://machinetranslate.org/vocabulary) 
Literally an ordered list of token frequency > index.

Byte-Pair Encoding used in GPTs (https://machinetranslate.org/byte-pair-encoding)

FastText, GloVe, Word2Vec
Almost all current NLP start w embedding layer
Very corpus dependent, can’t tell homophones 

https://chat.openai.com/share/ce3a7d7d-b466-4693-abc6-ae91d3cbfacb

Computer read, opaque slit to see N-words at a time and infer. Horse blinders. 

BLOOM 
Tokenization: The BLOOM tokenizer (link), a learned subword tokenizer trained using:

    A byte-level Byte Pair Encoding (BPE) algorithm
    A simple pre-tokenization rule, no normalization
    A vocabulary size of 250,680
    It was trained on a subset of a preliminary version of the corpus using alpha-weighting per language. 

WEAVER
Machine language, weaver presumed languages to have universal structure. “The common base of human communication”
WW: “it is very tempting to say that a book written in Chinese is simply a book written in English which was coded into the "Chinese
code." “
Called for “ statistical semantic studies should be undertaken, as a necessary
preliminary step. ”
Deep in the structures of language where the traits are common across.
“Translating the Bible,translation is like kissing your sweetheart through a veil.”


“What choice of adjacent words maximizes the probability of correct choice of meaning, and at the same time leads to a small value of N? “
“using the micro-context to settle the difficult cases of ambiguity”
“one lengthens the slit in the opaque mask, until one can see not only the central word in question, but also say N words on either side, then if N is large enough one can unambiguously decide the meaning of
the central word”


Hayles on Weaver & Shannon’s collab: 
“According to Eric A. Weiss, Shannon told him in correspondence that Weaver put together the volume without consulting Shannon. Weiss wrote: 'Weaver was a big-shot scientific gatekeeper at the time; Shannon was a more or less nobody. Weaver took some notes ... or something by Shannon and turned it into the 1949 writing putting his name first and without really getting Shannon's consent. Shannon felt that Weaver had made a good explanation, this was one ofWeaver's skills, and did not object seriously at the time" (Weiss to author, private communication).”

FISHER
eugenics journal, measurements into categories. Ronald Fisher’s 1936 paper “The use of multiple measurements in taxonomic problems”
Fisher's son in law was George E.P. BOX, another prominent statistician. Who got into the field exposing small animals to poisonous gas in the British army. He’s attributed the phrase **“all models are wrong but some are useful”**

CALDWELL's SINOTYPE
"In the course of his research, Caldwell made a second startling discovery. Not only did Chinese characters have a spelling, but, as he wrote, ‘the spelling of Chinese characters is highly redundant’. It was almost never necessary for Caldwell to enter every stroke within a character in order for the machine to retrieve it from memory. For a character containing 15 strokes, for example, it might only be necessary for the operator to enter the first five or six strokes before the Sinotype arrived at a positive match." [1]
In so doing he also invented the first auto-complete method of typing—one of the most widely used features of computing today.
“The Sinotype—A Machine for the Composition of Chinese from a Keyboard,” Journal of the Franklin Institute, 267 (1959) 471-502.


John WILKIN's universal language (https://historyofinformation.com/index.php?cat=29#entry_1553)

Kircher's attempts at translation and universal signs Ars Magna

Word Net 1985

RICHARD RICHENS SEMANTIC NETS (https://historyofinformation.com/detail.php?id=3633)

Nett & Hetzler, An Introduction to Electronic Data Processing￼ [1959] (86-88)

Neurath & ISOTYPES pectoral language
Gesellschaft und Wirtschaft : bildstatistisches Elementarwerk
https://www.digital.wienbibliothek.at/wbrobv/content/titleinfo/2295773
https://www.fulltable.com/iso/is03.htm

Yehoshua Bar-Hillel
Margaret Masterman
Jean Senellart

Georges Arzrouni French Armenian engineer, “mechanical brain 1933 for translation, word for word
“Dictionary of phrases”

Petr Petrovič Trojanskij 1933 patent
“Universal’ symbols for coding and interpreting grammatical functions, translating machine
Logical parsing symbols

McColloch–Pitts boolean > neuro 1943 “Representation of events in nerve nets and finite automata,”

Regular expressions 1951 Stephen Kleene 